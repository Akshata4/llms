{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9345477",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with LangChain\n",
    "\n",
    "This notebook demonstrates a RAG implementation using LangChain, OpenRouter (or OpenAI), and vector stores. RAG enhances Large Language Model (LLM) responses by:\n",
    "1. Retrieving relevant information from a knowledge base\n",
    "2. Augmenting the prompt with this context\n",
    "3. Generating a response using an LLM\n",
    "\n",
    "We'll break down each component and show how they work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a762da3",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we'll import the required libraries and set up our environment. We use:\n",
    "- `langchain` for the RAG pipeline components\n",
    "- `bs4` (BeautifulSoup4) for web scraping\n",
    "- `python-dotenv` for environment variable management\n",
    "- OpenRouter/OpenAI for embeddings and LLM capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import List, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import bs4\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41189bf1",
   "metadata": {},
   "source": [
    "## 2. API Configuration\n",
    "\n",
    "We'll set up the LLM and embeddings clients. We're using OpenRouter in this example, which is API-compatible with OpenAI but allows access to various models. The setup includes:\n",
    "- Loading API key from environment\n",
    "- Configuring LLM client with appropriate model and headers\n",
    "- Setting up embeddings client for vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99273023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key() -> str:\n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\n",
    "            \"OPENROUTER_API_KEY not found. Put it in a .env file, e.g.\\n\"\n",
    "            \"OPENROUTER_API_KEY=sk-or-... \"\n",
    "        )\n",
    "    return api_key\n",
    "\n",
    "api_key = get_api_key()\n",
    "\n",
    "# Configure LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    # optional but recommended by OpenRouter:\n",
    "    default_headers={\n",
    "        \"HTTP-Referer\": \"http://localhost\",\n",
    "        \"X-Title\": \"LangChain RAG Demo\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Configure embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"openai/text-embedding-3-large\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3627131",
   "metadata": {},
   "source": [
    "## 3. Document Loading and Chunking\n",
    "\n",
    "For RAG to work effectively, we need to:\n",
    "1. Load documents from a source (in this case, a web page)\n",
    "2. Split them into smaller chunks for better retrieval\n",
    "3. Ensure the chunks have enough context but aren't too large\n",
    "\n",
    "The `load_and_chunk` function handles this process using:\n",
    "- `WebBaseLoader` for fetching and parsing web content\n",
    "- `RecursiveCharacterTextSplitter` for smart document chunking\n",
    "- BeautifulSoup for robust HTML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ec51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk(url: str) -> List[Document]:\n",
    "    \"\"\"Load content from a URL and split it into chunks for RAG.\"\"\"\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(name=True)  # Parse whole body robustly\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split into chunks with some overlap for context preservation\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# Example: Load the LangChain RAG tutorial\n",
    "url = \"https://python.langchain.com/docs/tutorials/rag/\"\n",
    "chunks = load_and_chunk(url)\n",
    "print(f\"Loaded {len(chunks)} chunks from {url}\")\n",
    "\n",
    "# Show a sample chunk\n",
    "if chunks:\n",
    "    print(\"\\nSample chunk content:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(chunks[0].page_content[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1e057",
   "metadata": {},
   "source": [
    "## 4. Vector Store Creation\n",
    "\n",
    "Now we'll create a vector store from our document chunks. This involves:\n",
    "1. Converting text chunks into vector embeddings\n",
    "2. Storing them in a vector database (in-memory for this demo)\n",
    "3. Enabling similarity search for retrieval\n",
    "\n",
    "The `index_docs` function handles this process using:\n",
    "- OpenAI's text-embedding-3-large model for embeddings\n",
    "- LangChain's InMemoryVectorStore for storage and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple fallback embeddings for testing without API\n",
    "class SimpleFallbackEmbeddings:\n",
    "    \"\"\"Local fallback embeddings that work offline.\"\"\"\n",
    "    def __init__(self, dim: int = 16):\n",
    "        self.dim = dim\n",
    "\n",
    "    def _text_to_vector(self, text: str):\n",
    "        import hashlib\n",
    "        h = hashlib.sha256(text.encode(\"utf-8\")).digest()\n",
    "        return [(h[i % len(h)] / 255.0) * 2 - 1 for i in range(self.dim)]\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self._text_to_vector(t or \"\") for t in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self._text_to_vector(text or \"\")\n",
    "\n",
    "def index_docs(chunks: List[Document], embeddings: OpenAIEmbeddings):\n",
    "    \"\"\"Create a searchable vector store from document chunks.\"\"\"\n",
    "    fallback_used = False\n",
    "    try:\n",
    "        # Test embeddings with a small sample\n",
    "        sample_texts = [c.page_content for c in chunks[:2]] if chunks else []\n",
    "        sample_vectors = embeddings.embed_documents(sample_texts) if sample_texts else []\n",
    "        if sample_vectors and (not isinstance(sample_vectors, list) or not hasattr(sample_vectors[0], \"__len__\")):\n",
    "            raise TypeError(f\"Unexpected embeddings response type: {type(sample_vectors)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: remote embeddings failed ({repr(e)}). Falling back to local embeddings.\")\n",
    "        embeddings = SimpleFallbackEmbeddings()\n",
    "        fallback_used = True\n",
    "\n",
    "    vs = InMemoryVectorStore(embeddings)\n",
    "    vs.add_documents(chunks)\n",
    "    return vs\n",
    "\n",
    "# Create vector store from our chunks\n",
    "vector_store = index_docs(chunks, embeddings)\n",
    "\n",
    "# Test similarity search\n",
    "query = \"What is RAG?\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "print(f\"\\nTest query: {query}\")\n",
    "print(\"\\nTop 2 most relevant chunks:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. \" + \"=\" * 40)\n",
    "    print(doc.page_content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d78107",
   "metadata": {},
   "source": [
    "## 5. RAG Pipeline Construction\n",
    "\n",
    "Now we'll build the complete RAG pipeline using LangGraph. The pipeline has two main steps:\n",
    "1. **Retrieve**: Find relevant chunks based on the question\n",
    "2. **Generate**: Create an answer using the LLM and retrieved context\n",
    "\n",
    "We use:\n",
    "- LangGraph for pipeline orchestration\n",
    "- A slim RAG prompt from LangChain's hub\n",
    "- TypedDict for type-safe state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"Type-safe state management for the RAG pipeline.\"\"\"\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def build_graph(vector_store: InMemoryVectorStore, llm: ChatOpenAI):\n",
    "    \"\"\"Build the RAG pipeline as a graph of operations.\"\"\"\n",
    "    # Get the slim RAG prompt from the Hub\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    def retrieve(state: RAGState):\n",
    "        \"\"\"Find relevant documents for the question.\"\"\"\n",
    "        retrieved = vector_store.similarity_search(state[\"question\"], k=4)\n",
    "        return {\"context\": retrieved}\n",
    "\n",
    "    def generate(state: RAGState):\n",
    "        \"\"\"Generate an answer using the LLM and context.\"\"\"\n",
    "        ctx_text = \"\\n\\n\".join(d.page_content for d in state[\"context\"])\n",
    "        msgs = prompt.invoke({\"question\": state[\"question\"], \"context\": ctx_text})\n",
    "        out = llm.invoke(msgs)\n",
    "        return {\"answer\": out.content}\n",
    "\n",
    "    # Build and compile the graph\n",
    "    graph = StateGraph(RAGState)\n",
    "    graph.add_node(\"retrieve\", retrieve)\n",
    "    graph.add_node(\"generate\", generate)\n",
    "    graph.add_edge(START, \"retrieve\")\n",
    "    graph.add_edge(\"retrieve\", \"generate\")\n",
    "    return graph.compile()\n",
    "\n",
    "# Create the RAG pipeline\n",
    "graph = build_graph(vector_store, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4d35b",
   "metadata": {},
   "source": [
    "## 6. Using the RAG Pipeline\n",
    "\n",
    "Finally, we'll use our RAG pipeline to answer questions. The pipeline will:\n",
    "1. Take a question as input\n",
    "2. Retrieve relevant context from our indexed documents\n",
    "3. Generate an answer using the LLM and context\n",
    "4. Show which documents were used as sources\n",
    "\n",
    "Let's try it with a few example questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb277d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(question: str):\n",
    "    \"\"\"Ask a question using the RAG pipeline and show results.\"\"\"\n",
    "    print(\"=== QUESTION ===\")\n",
    "    print(question)\n",
    "    \n",
    "    result = graph.invoke({\"question\": question})\n",
    "    \n",
    "    print(\"\\n=== ANSWER ===\")\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    print(\"\\n=== TOP CONTEXT SOURCES ===\")\n",
    "    for i, d in enumerate(vector_store.similarity_search(question, k=3), start=1):\n",
    "        src = d.metadata.get(\"source\") or d.metadata.get(\"loc\") or \"unknown\"\n",
    "        print(f\"{i}. {src}\")\n",
    "\n",
    "# Try some example questions\n",
    "questions = [\n",
    "    \"What are the basic steps in RAG?\",\n",
    "    \"How does RAG improve LLM responses?\",\n",
    "    \"What are some common challenges with RAG systems?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    ask_rag(q)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
